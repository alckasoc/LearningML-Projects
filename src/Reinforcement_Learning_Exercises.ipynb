{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning Exercises.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2crU2TaTK-8H"
      },
      "source": [
        "[Reinforcement Learning TF-Agents](https://colab.research.google.com/drive/1FXh1BQgMI5xE1yIV1CQ25TyRVcxvqlbH?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZVtEcpWiZeJ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "# nice plot figures\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "import matplotlib.animation as animation\n",
        "# smooth animations\n",
        "mpl.rc('animation', html='jshtml')\n",
        "\n",
        "import PIL\n",
        "import os\n",
        "\n",
        "import gym\n",
        "import tf_agents\n",
        "from tf_agents.environments import suite_atari, suite_gym\n",
        "\n",
        "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
        "from tf_agents.environments.atari_wrappers import FrameStack4\n",
        "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
        "\n",
        "from tf_agents.networks.q_network import QNetwork\n",
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
        "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "from tf_agents.utils.common import function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibdY52A0j55N"
      },
      "source": [
        "# functions to plot animations on a per frame basis\n",
        "def update_scene(num, frames, patch):\n",
        "    patch.set_data(frames[num])\n",
        "    return patch,\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):\n",
        "    fig = plt.figure()\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, update_scene, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()\n",
        "    return anim\n",
        "\n",
        "# save an agent's demo (after training)\n",
        "saved_frames = []\n",
        "def save_frames(trajectory):\n",
        "    global saved_frames\n",
        "    saved_frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
        "\n",
        "def play_game_demo(tf_env, the_agent, obs_list, n_steps):\n",
        "    watch_driver = DynamicStepDriver(\n",
        "        tf_env,\n",
        "        the_agent.policy,\n",
        "        observers=[save_frames] + obs_list,\n",
        "        num_steps=n_steps)\n",
        "    final_time_step, final_policy_state = watch_driver.run()\n",
        "\n",
        "def save_animated_gif(frames): # saved_frames is passed in\n",
        "    image_path = os.path.join(\"images\", \"rl\", \"breakout.gif\")\n",
        "    frame_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\n",
        "    frame_images[0].save(image_path, format='GIF',\n",
        "                         append_images=frame_images[1:],\n",
        "                         save_all=True,\n",
        "                         duration=30,\n",
        "                         loop=0)\n",
        "    \n",
        "# %%html\n",
        "# <img src=\"images/rl/breakout.gif\" /> runs the gif in a jupyter/colab environment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGsE-NnHkKNE"
      },
      "source": [
        "# 8\n",
        "\n",
        "# install this dependency for LunarLander\n",
        "# pip install gym[box2d] \n",
        "test_env = gym.make(\"LunarLander-v2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVaa7RRQnyub",
        "outputId": "fe2107a5-8a75-4f2a-9813-2a832c37d245"
      },
      "source": [
        "test_env # seems like there is a time limit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TimeLimit<LunarLander<LunarLander-v2>>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awafewULn_j7",
        "outputId": "ec9fedd6-40b3-4b1d-f3c6-312fc89e982d"
      },
      "source": [
        "test_env.reset() # 8 values from each observation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.00610943,  1.4206164 ,  0.6188056 ,  0.43093634, -0.00707254,\n",
              "       -0.14016892,  0.        ,  0.        ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lqwRxMtoY1D"
      },
      "source": [
        "\n",
        "From the source code, we can see that these each 8D observation (x, y, h, v, a, w, l, r) correspond to:\n",
        "\n",
        "+ x,y: the coordinates of the spaceship. It starts at a random location near (0, 1.4) and must land near the target at (0, 0).\n",
        "+ h,v: the horizontal and vertical speed of the spaceship. It starts with a small random speed.\n",
        "+ a,w: the spaceship's angle and angular velocity.\n",
        "+ l,r: whether the left or right leg touches the ground (1.0) or not (0.0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdmz4QQvoBUa",
        "outputId": "a7a399c6-ccfe-4f6d-dd09-d20ae4b0b19a"
      },
      "source": [
        "print(test_env.observation_space) # \n",
        "print(test_env.action_space, test_env.action_space.n) # 4 possible values\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Box(-inf, inf, (8,), float32)\n",
            "Discrete(4) 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvGo8cYVpltS"
      },
      "source": [
        "Looking at the https://gym.openai.com/envs/LunarLander-v2/, these actions are:\n",
        "\n",
        "+ do nothing\n",
        "+ fire left orientation engine\n",
        "+ fire main engine\n",
        "+ fire right orientation engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byRDntL9po78"
      },
      "source": [
        "# PG REINFORCE algorithm\n",
        "\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "n_inputs = test_env.observation_space.shape[0]\n",
        "n_outputs = test_env.action_space.n\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  keras.layers.Dense(32, activation=\"relu\", input_shape=[n_inputs]),\n",
        "  keras.layers.Dense(32, activation='relu'),\n",
        "  keras.layers.Dense(32, activation='relu'),\n",
        "  keras.layers.Dense(n_outputs, activation=\"softmax\")                                                        \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haY_v4YUqfyJ"
      },
      "source": [
        "# play multiple episodes, exploring the environment randomly and recording \n",
        "# gradients and rewards\n",
        "\n",
        "def play_one_step(env, obs, model, loss_fn):\n",
        "  with tf.GradientTape() as tape:\n",
        "    probas = model(obs[np.newaxis])\n",
        "    logits = tf.math.log(probas + keras.backend.epsilon())\n",
        "    action = tf.random.categorical(logits, num_samples=1)\n",
        "    loss = tf.reduce_mean(loss_fn(action, probas))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  return obs, reward, done, grads\n",
        "\n",
        "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
        "  all_grads, all_rewards = [], []\n",
        "  for episode in range(n_episodes):\n",
        "    current_grads, current_rewards = [], []\n",
        "    obs = env.reset()\n",
        "    for step in range(n_max_steps):\n",
        "      obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
        "      current_rewards.append(reward)\n",
        "      current_grads.append(grads)\n",
        "      if done:\n",
        "        break\n",
        "  all_grads.append(current_grads)\n",
        "  all_rewards.append(current_rewards)\n",
        "  return all_rewards, all_grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXFU5tfHsdL7"
      },
      "source": [
        "# compute sum of future discounted rewards and standardize to differentiate\n",
        "# good and bad decisions\n",
        "\n",
        "def discount_rewards(discounted, discount_rate):\n",
        "  discounted = np.array(discounted)\n",
        "  for step in range(len(discounted) - 2, -1, -1):\n",
        "    discounted[step] += discounted[step + 1] * discount_rate\n",
        "  return discount\n",
        "\n",
        "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
        "  discounted_rewards = [discount_rewards(reward, discount_rate) for reward in all_rewards]\n",
        "  flattened_rewards = np.concatenate(discounted_rewards)\n",
        "  rewards_mean = flattened_rewards.mean()\n",
        "  rewards_stddev = flattened_rewards.std()\n",
        "  return [(reward - rewards_mean) / rewards_stddev for reward in discounted_rewards]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLKYDSI8snXp"
      },
      "source": [
        "n_iterations = 200\n",
        "n_episodes_per_update = 16\n",
        "n_max_steps = 1000\n",
        "discount_rate = 0.99"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vehhB5LxteeR",
        "outputId": "29797c8f-cc92-4c24-c017-9b56386192a5"
      },
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "optimizer = keras.optimizers.Nadam(lr=0.005)\n",
        "loss_fn = keras.losses.sparse_categorical_crossentropy\n",
        "# the model outputs probabilities for each class so we use categorical_crossentropy\n",
        "# and the action is just 1 value (not a 1 hot vector so we use sparse_categorical_crossentropy)\n",
        "env.seed(42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[42]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_2_zjGJtpGi"
      },
      "source": [
        "# this will take very long, so I'm not calling it for the sake of my computer's mental health\n",
        "def train(n_iterations, env, n_episodes_per_update, n_max_steps, model, loss_fn, discount_rate):\n",
        "  for iteration in range(n_iterations):\n",
        "    all_rewards, all_grads = play_multiple_episodes(env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
        "\n",
        "    # for plotting the learning curve with undiscounted rewards\n",
        "    # alternatively, just use a reduce_sum from tf and extract the numpy scalar value using .numpy()\n",
        "    mean_reward = sum(map(sum, all_rewards)) / n_episodes_per_update\n",
        "    print(\"\\rIteration: {}/{}, mean reward: {:.1f}  \".format( # \\r means that it will not return a new line, it will just replace the current line\n",
        "            iteration + 1, n_iterations, mean_reward), end=\"\")\n",
        "    mean_rewards.append(mean_reward)\n",
        "\n",
        "    all_discounted_rewards = discount_and_normalize_rewards(all_rewards, discount_rate)\n",
        "    all_mean_grads = []\n",
        "    for var_index in range(len(model.trainable_variables)):\n",
        "      mean_grads = tf.reduce_mean(\n",
        "              [final_reward * all_grads[episode_index][step][var_index]\n",
        "              for episode_index, final_rewards in enumerate(all_discounted_rewards)\n",
        "                  for step, final_reward in enumerate(final_rewards)], axis=0)\n",
        "    all_mean_grads.append(mean_grads)\n",
        "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJI2JEJVvibp",
        "outputId": "37ab0402-824a-40b0-db92-e6ba95be7508"
      },
      "source": [
        "# 9 TF-Agents SpaceInvaders-v4\n",
        "\n",
        "environment_name = \"SpaceInvaders-v4\"\n",
        "env = suite_atari.load(\n",
        "    environment_name,\n",
        "    max_episode_steps=27000,\n",
        "    gym_env_wrappers=[AtariPreprocessing, FrameStack4]\n",
        ")\n",
        "env"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_agents.environments.atari_wrappers.AtariTimeLimit at 0x7f7f337f0950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLgJMiLy55Lm"
      },
      "source": [
        "+ environment ✓\n",
        "+ driver ✓\n",
        "+ observer(s) ✓\n",
        "+ replay buffer ✓\n",
        "+ dataset ✓\n",
        "+ agent with collect policy ✓\n",
        "+ DQN ✓\n",
        "+ training loop ✓"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iGYyNEq3gbg"
      },
      "source": [
        "# environment officially built\n",
        "tf_env = TFPyEnvironment(env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCrbdj4d51eR"
      },
      "source": [
        "dropout_params = [0.4]\n",
        "fc_params = [512]\n",
        "conv_params = [(32, (8, 8), 5),\n",
        "               (64, (4, 4), 4),\n",
        "               (64, (3, 3), 1),]\n",
        "preprocessing_layer = keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32) / 255.) # uint8 beforehand\n",
        "\n",
        "dqn = QNetwork(\n",
        "    tf_env.observation_spec(),\n",
        "    tf_env.action_spec(),\n",
        "    preprocessing_layers=preprocessing_layer,\n",
        "    conv_layer_params=conv_params,\n",
        "    fc_layer_params=fc_params,\n",
        "    dropout_layer_params=dropout_params,\n",
        "    activation_fn=keras.activations.relu,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wApunj78Aau"
      },
      "source": [
        "# dqn agent with collect policy officially built\n",
        "update_period = 4\n",
        "train_step = tf.Variable(0)\n",
        "epsilon_greedy_policy = keras.optimizers.schedules.PolynomialDecay( \n",
        "    initial_learning_rate=1.0, \n",
        "    decay_steps=250000 // update_period, \n",
        "    end_learning_rate=0.01, \n",
        ")\n",
        "\n",
        "dqn_agent = DqnAgent(\n",
        "    tf_env.time_step_spec(),\n",
        "    tf_env.action_spec(),\n",
        "    q_network=dqn,\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False),\n",
        "    train_step_counter=train_step,\n",
        "    gamma=0.99, \n",
        "    td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
        "    target_update_period=2000,\n",
        "    epsilon_greedy=lambda: epsilon_greedy_policy(train_step)\n",
        ")\n",
        "dqn_agent.initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g-jQ5u39irF"
      },
      "source": [
        "# uniform replay buffer officially built\n",
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    dqn_agent.collect_data_spec,\n",
        "    batch_size = tf_env.batch_size,\n",
        "    max_length=100000,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZUwRQNC99dr"
      },
      "source": [
        "replay_buffer_observer = replay_buffer.add_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjUOD4tJHHeS"
      },
      "source": [
        "# observers + metrics officially built\n",
        "training_metrics = [\n",
        "  tf_metrics.AverageEpisodeLengthMetric(),\n",
        "  tf_metrics.AverageReturnMetric(),\n",
        "  tf_metrics.NumberOfEpisodes(),\n",
        "  tf_metrics.EnvironmentSteps(),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lJhjzyDHdTb"
      },
      "source": [
        "class ShowProgress:\n",
        "    def __init__(self, total):\n",
        "        self.counter = 0\n",
        "        self.total = total\n",
        "    def __call__(self, trajectory):\n",
        "        if not trajectory.is_boundary(): \n",
        "            self.counter += 1\n",
        "        if self.counter % 100 == 0:\n",
        "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")\n",
        "\n",
        "# driver officially created\n",
        "driver = DynamicStepDriver(\n",
        "    tf_env, \n",
        "    dqn_agent.collect_policy,\n",
        "    observers = training_metrics + [ShowProgress(2000)],\n",
        "    num_steps=update_period\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id0heRXGIK2T",
        "outputId": "6965d539-6291-48ae-81bc-34f05da71a51"
      },
      "source": [
        "random_policy = RandomTFPolicy(\n",
        "    tf_env.time_step_spec(),\n",
        "    tf_env.action_spec()\n",
        ")\n",
        "\n",
        "initial_driver = DynamicStepDriver(\n",
        "    tf_env, \n",
        "    random_policy,\n",
        "    observers = [replay_buffer.add_batch] + [ShowProgress(2000)],\n",
        "    num_steps=update_period\n",
        ")\n",
        "\n",
        "final_time_step, final_policy_state = initial_driver.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_agents/drivers/dynamic_step_driver.py:203: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.while_loop(c, b, vars, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BS90vJQImby",
        "outputId": "2be0bf2e-6795-4f93-bcb1-305457133c89"
      },
      "source": [
        "# dataset officially built\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    sample_batch_size=64,\n",
        "    num_steps=2,\n",
        "    num_parallel_calls=3, \n",
        ").prefetch(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/operators/control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqwPeDoOI3Zz"
      },
      "source": [
        "driver.run = function(driver.run)\n",
        "dqn_agent.train = function(dqn_agent.train)\n",
        "\n",
        "# I would train it, but my computer suffers from dementia\n",
        "# training loop officially built\n",
        "def training(n_iterations, agent, driver, tf_env, dataset):\n",
        "  time_step = None\n",
        "  initial_policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
        "  iterator = iter(dataset) # forgot to do this!\n",
        "  for iteration in range(n_iterations):\n",
        "    time_step, policy_state = driver.run(time_step, policy_state)\n",
        "    trajectories, buffer_info = next(iterator)\n",
        "    train_loss = agent.train(trajectories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66KTIx0yKXOY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}